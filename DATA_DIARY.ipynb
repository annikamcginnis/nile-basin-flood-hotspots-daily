{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9967bf5-a502-4827-82ec-b20aa155e01a",
   "metadata": {},
   "source": [
    "# **Nile Basin Flash Flood Hotspots Map: How I did it**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7556067d-436b-457f-a3d4-d95368b9574a",
   "metadata": {},
   "source": [
    "### By Annika McGinnis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f2b49d-7a5b-4877-bf38-fb16102a1816",
   "metadata": {},
   "source": [
    "My goal in this project was to **scrape and visualize data on flooding in the Nile Basin**, which is mapped on a daily basis on the Nile Basin Flash Flood Early Warning System by the Nile Basin Initiative: https://flashfloodalert.nilebasin.org/workspaces/6c407e1b-5d25-4d83-b782-b6c81f8648ee/applications/9a806474-3cdd-447a-8a01-9898d9974bf8. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc2488f-19a5-4bba-a6b3-dc020b540f0c",
   "metadata": {},
   "source": [
    "# Project Attempt #1: Many unsuccessful attempts to download administrative regions data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc3cf0b-653c-4c87-9a80-ae2a1984f5a0",
   "metadata": {},
   "source": [
    "First, I tried scraping the **administrative units layer** of the geodatabase, which shows daily flood risk by sub-national administrative region for 9 countries in the Nile Basin. Inspecting the page code in the Network tab, I opened the API link being loaded on the map for the administrative units data. I realized this data was being shown as WMS images like this one: https://nilebasin-dss-data.azurewebsites.net/geoserver/nile2/wms?TRANSPARENT=true&VERSION=1.3.0&REQUEST=GetMap&FORMAT=image/png&CRS=EPSG:3857&SERVICE=WMS&HEIGHT=865&WIDTH=452&LAYERS=admin_cat,hybas_cat&BBOX=2605661.647053178,-630111.515001775,4852508.670218566,3669717.4120734744&TIME=2024-12-13T00:00:00.000Z "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a500ac6e-02e9-4174-a3ff-e7b90bf50c48",
   "metadata": {},
   "source": [
    "Using chatGPT with the help of my professor, I was able to devise the **WFS (Web Feature Service) link** for the geodatabase from the WMS (Web Map Service) URL. Through this, I also accessed the *WFS Capabilities URL*, which lists all the feature names and descriptions for the map: https://nilebasin-dss-data.azurewebsites.net/geoserver/nile2/wfs?SERVICE=WFS&REQUEST=GetCapabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620ffd29-14a5-48ba-974a-f3b514234c3a",
   "metadata": {},
   "source": [
    "I tried downloading data from the WFS URL by modifying the URL with one of the features, specifically by adding the <Name> listing for the nile2:country_categories feature in the < FeatureTypeList >. Through visiting the URL, I was able to download the JSON in my browser and on my machine. I viewed it in mapshaper.org and geojson.io and realized the current bounding box was limiting it to one country. I removed the bounding box and confirmed that the file seemed to include all countries. \n",
    "\n",
    "_This is the URL I modified to download this layer: https://nilebasin-dss-data.azurewebsites.net/geoserver/nile2/wfs?SERVICE=WFS&VERSION=1.1.0&REQUEST=GetFeature&TYPENAME=nile2:country_categories&SRSNAME=EPSG:4326&OUTPUTFORMAT=application/json_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87348da5-6d69-4921-bf10-384d6765949a",
   "metadata": {},
   "source": [
    "I then identified the layer containing flood risk data by administrative regions, which is called **nile2:admin_cat**\n",
    "\n",
    "_I modified the URL as: https://nilebasin-dss-data.azurewebsites.net/geoserver/nile2/wfs?SERVICE=WFS&VERSION=1.1.0&REQUEST=GetFeature&TYPENAME=nile2:admin_cat&SRSNAME=EPSG:4326&OUTPUTFORMAT=application/json_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447205e2-f566-4426-96be-954df0cd1065",
   "metadata": {},
   "source": [
    "For about a week, I made many unsuccessful attempts to download this layer using the requests library, because of its large size. \n",
    "- Pagination did not work for this particular URL.\n",
    "- Adding a bounding box was irrelevant because I needed all the geographic data on the layer.\n",
    "- Code that attempted to run the API request in a loop until it worked failed repeatedly. \n",
    "\n",
    "However, I know the URL was valid, because when I added *MAXFEATURES* to the URL, I was sometimes able to download parts of the data, which I verified by uploading it into mapshaper. I ran the below code many times over the course of two days and was able to download up to 200,000 features, but never the full dataset. The number of features I could download differed at various times of day; sometimes it failed completely and sometimes I could download only up to 10,000 features. It's important to note as well that after downloading the data from the API, this code simplifies geometries to a tolerance of 0.01, which also reduces the file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1721e6bc-16b7-41a9-9eda-9fd2c77c9857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import geopandas as gpd\n",
    "import io\n",
    "\n",
    "# URL to request WFS data\n",
    "wfs_url = (\n",
    "    \"https://nilebasin-dss-data.azurewebsites.net/geoserver/nile2/wfs?\"\n",
    "    \"SERVICE=WFS&VERSION=1.1.0&REQUEST=GetFeature&TYPENAME=nile2:admin_cat\"\n",
    "    \"&SRSNAME=EPSG:4326&OUTPUTFORMAT=application/json\"\n",
    "    \"&MAXFEATURES=10000\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Make a GET request to the WFS URL\n",
    "    response = requests.get(wfs_url, stream=True)\n",
    "    response.raise_for_status()  # Raise an error for bad responses\n",
    "\n",
    "    # Load the data into a GeoDataFrame if the request is successful\n",
    "    gdf = gpd.read_file(io.BytesIO(response.content))\n",
    "\n",
    "    # Simplify the geometries (adjust tolerance as needed)\n",
    "    gdf['geometry'] = gdf['geometry'].simplify(tolerance=0.01, preserve_topology=True)\n",
    "\n",
    "    # Save the simplified GeoDataFrame to a GeoJSON file\n",
    "    gdf.to_file(\"simplified_admin_cat3.geojson\", driver=\"GeoJSON\")\n",
    "    print(\"Simplified GeoJSON file saved.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Request failed: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd46a265-415c-4ae6-9983-b3c1b03b4f1b",
   "metadata": {},
   "source": [
    "## Simplifying and visualizing one large downloaded file of administrative regions data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79de751b-a3c4-43bc-b20d-60f97b102edb",
   "metadata": {},
   "source": [
    "Although the data couldn't be downloaded via requests or in the browser, I was able to download one of the files by clicking File -> Save Page As while the URL was loading. \n",
    "\n",
    "The file size was 8.95 GB. It couldn't be opened in VisualStudioCode and took about 8 hours to open in Sublime Text, but froze so I was unable to scroll through it. I downloaded QGIS and was able to load the main URL and see the options for layers, but it also froze when I try to load any layer. I also tried to view smaller segments of the data using Geopandas in a Python Jupyter Notebook, but this also didnâ€™t work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa9cfe-36d1-4b8c-8bb7-ae5de46227f2",
   "metadata": {},
   "source": [
    "Next, I used Python in a Jupyter Notebook to load this large JSON file from my computer and reduce it to around 400 MB by simplifying geometries. This is the code I used: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945af443-e67e-4c5e-83aa-faf26f9877fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "input_file = \"PATH_TO_MY_FILE\"\n",
    "output_file = \"simplified_admin_cat.json\"\n",
    "\n",
    "print(\"Loading GeoJSON file...\")\n",
    "gdf = gpd.read_file(input_file)\n",
    "\n",
    "print(\"Simplifying geometries...\")\n",
    "gdf[\"geometry\"] = gdf[\"geometry\"].simplify(tolerance=0.01, preserve_topology=True)\n",
    "\n",
    "print(f\"Saving simplified GeoJSON to {output_file}...\")\n",
    "gdf.to_file(output_file, driver=\"GeoJSON\")\n",
    "\n",
    "print(\"Simplification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a0dfab-087a-4f2d-a5b6-43810a315718",
   "metadata": {},
   "source": [
    "After this, I loaded the file into QGIS, checked to see it captured all regions, simplified geometries a bit more and reduced the decimal points of coordinates (set at 15) to 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf08ad2-4064-449e-a113-deac7d927f4c",
   "metadata": {},
   "source": [
    "Since this file was still huge, it could not be uploaded to Datawrapper, and simplifying geometries more would make regions unrecognizable. I looked up other ways to present the data and came across using **tiles**.\n",
    "\n",
    "I used tippecanoe to convert the simplified Geojson file (still over 400 MB) to a mbtiles file (18 MB):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58dc45-0c67-49bd-9e1f-3c3d713c9d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tippecanoe -o admin_cat.mbtiles PATH_TO_MY_FILE.geojson\n",
    "\n",
    "map.on('load', function () {\n",
    "    map.addSource('vector-tiles', {\n",
    "        type: 'vector',\n",
    "        url: ''PATH_TO_MY_FILE'' // URL to vector tiles\n",
    "    });\n",
    "    map.addLayer({\n",
    "        'id': 'vector-layer',\n",
    "        'type': 'fill',\n",
    "        'source': 'vector-tiles',\n",
    "        'source-layer': 'layer-name',\n",
    "        'paint': {\n",
    "            'fill-color': '#888888',\n",
    "            'fill-opacity': 0.4\n",
    "        }\n",
    "    });\n",
    "});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ffb59a-56af-4d0a-808d-493875238439",
   "metadata": {},
   "source": [
    "I attempted to create a mapbox tile layer using code, all of which failed, so I created a Mapbox Studio account, uploaded the mbtiles files there and then got the tile ID that way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2964f22-3c37-43b7-93eb-ee3dcfc3f336",
   "metadata": {},
   "source": [
    "I created an HTML file linking to the mapbox tile ID, and customized it to create this interactive map (see attached file: **mapbox_nilebasinfloodmap.html**). Although all regions were shown, flood risk values for certain regions could only be seen when zooming in. Also, the map could not be automatically updated. \n",
    "\n",
    "**At this point, I decided to switch to downloading the *hotspots layer* instead, which shows similar information on flood risk for certain locations but is a much smaller file mapping points rather than geographic regions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f8585b-f24a-45f3-a134-84bd7a2711c9",
   "metadata": {},
   "source": [
    "I decided to try to use the geographic regions data to come up with a boundary for this map. Back in QGIS with my simplified admin_cat file, I dissolved regions and saved the polygon lines in QGIS. Then I exported it as a Geojson file to get the overall boundary shape. I imported this polygon to Datawrapper, but the map was unable to detect any other locations such as countries or cities like it can for other basemaps. Because it would be hard for viewers to understand where this polygon was located in the world and Datawrapper can only allow for one base map at a time, I decided not to use it, but I uploaded it to my final repo for potential use in layering for more complex maps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec06d3d3-318a-42d2-888b-84cb2c6a9a1f",
   "metadata": {},
   "source": [
    "# Project Attempt #2: Downloading and visualizing hotspots layer data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ebfc01-f8f1-4353-baf9-2f31fc8d6ae1",
   "metadata": {},
   "source": [
    "Using this layer was much more straightforward due to its manageable size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37396b47-6610-4a2d-8ad0-6d4199c1e79e",
   "metadata": {},
   "source": [
    "## Step 1: Scraping Flood Risk Geodatabase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4788a7c5-f1a9-4f3b-96ce-99138afee9ab",
   "metadata": {},
   "source": [
    "In \"hotspot_clusters_updater.ipynb,\" I download the layer **nile2:hybas_hotsport_cluster**, which contains all the information for the hotspots, using requests. This is the code I used: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d2ee8-b517-443c-b3a4-c9182ed216d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://nilebasin-dss-data.azurewebsites.net/geoserver/nile2/wfs?SERVICE=WFS&VERSION=1.1.0&REQUEST=GetFeature&TYPENAME=nile2:hybas_hotsport_cluster&SRSNAME=EPSG:4326&OUTPUTFORMAT=application/json\"\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff99ee75-9c48-4457-a066-72ca600e6c31",
   "metadata": {},
   "source": [
    "Although this works, it doesn't work all the time (probably due to the high-volume request to the server). So I revised the code to define a function that would run this code up to 10 times until the response came through. It allows for a two-second time break between tries using time.sleep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f31e489-5ea0-47cf-b9f9-fd73f913ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "url = \"https://nilebasin-dss-data.azurewebsites.net/geoserver/nile2/wfs?SERVICE=WFS&VERSION=1.1.0&REQUEST=GetFeature&TYPENAME=nile2:hybas_hotsport_cluster&SRSNAME=EPSG:4326&OUTPUTFORMAT=application/json\"\n",
    "\n",
    "def get_api_response(url, max_retries=10):\n",
    "    attempt = 0  \n",
    "    while attempt < max_retries:\n",
    "        attempt += 1  \n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response  \n",
    "        else:\n",
    "            time.sleep(2)  \n",
    "\n",
    "    return None  \n",
    "\n",
    "response = get_api_response(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c95520-4e98-46e0-93b4-bdb1d488e2ac",
   "metadata": {},
   "source": [
    "I checked how many of the same location (info for different days) were in the dataset by using one location. I found out there was data for 533 different days in this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaa4a14-2cb5-4e22-8c3b-ffb9e8a7f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_location = []\n",
    "\n",
    "for feature in data['features']:\n",
    "    if feature['geometry']['coordinates'] == [33.3816, 19.5028]:\n",
    "        first_location.append(feature['properties']['date_time'])\n",
    "\n",
    "len(first_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065555f3-3647-4b59-957f-d10505f11b26",
   "metadata": {},
   "source": [
    "I made sure the last day includes today's information by running this code: first_location[-1] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866a4c5f-d367-48b2-8dae-aad25959f3db",
   "metadata": {},
   "source": [
    "I tried this out for a couple of other locations to ensure that they were also represented the same number of times in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd0daeb-5471-4ad2-b49c-7bfa9fd245eb",
   "metadata": {},
   "source": [
    "From the JSON data received as the response to my API call, I extracted IDs, coordinates, flood risk and dates for each hotspot and added them to a list of dictionaries in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fc9aaf-c5b3-4d30-a4d1-a285058ccd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hotspots = []\n",
    "\n",
    "for feature in data['features']:\n",
    "    hotspot_dict = {}\n",
    "    hotspot_dict['id'] = feature['id']\n",
    "    hotspot_dict['coordinates'] = feature['geometry']['coordinates']\n",
    "    hotspot_dict['flood_risk'] = feature['properties']['max']\n",
    "    hotspot_dict['date'] = feature['properties']['date_time']\n",
    "    all_hotspots.append(hotspot_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93664f1a-e424-4240-9a16-7b43709def86",
   "metadata": {},
   "source": [
    "I imported this list of dictionaries to Pandas as a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c04563-cb98-487e-a66f-59b887f94ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.json_normalize(all_hotspots)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35034068-0872-44ba-bcfd-e928267107dd",
   "metadata": {},
   "source": [
    "I saved this data as a CSV: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99914d74-12f6-493d-9199-7f083cf28ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"all_hotspots.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28acb9f8-3f70-4ab5-9739-e06f2a9d82e4",
   "metadata": {},
   "source": [
    "## Step 2: Cleaning and Analyzing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a776832-556a-4b72-a3e0-428a2bbbcd68",
   "metadata": {},
   "source": [
    "I checked the most recent date in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07595df5-42e8-4ede-b09c-a1d33ef674ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='date', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258300d1-39ab-4794-9f96-804690b9ee49",
   "metadata": {},
   "source": [
    "I determined the total number of hotspots, which is currently 50: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7e498f-20ec-4f41-b6fa-bfc00e1a1f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['date'] == '2024-12-07T00:00:00Z']['date'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388f1f21-4eb9-4c0c-844a-db60d3be57b8",
   "metadata": {},
   "source": [
    "I cleaned the date column using regular expressions to remove the letters and digits after the date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817eeb43-a048-43ee-be30-d1b33cfe0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df['date'].astype(str)\n",
    "df['date'] = df['date'].str.extract(r'(\\d{4}[-]\\d{2}[-]\\d{2}).+')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b39af-dda5-4cc7-bbd6-be792be4b35b",
   "metadata": {},
   "source": [
    "I noticed each ID was different, even for the same locations. I tried to split them with regex so that the IDs can be grouped by location. But in doing so, I realized even the end parts aren't consistent for each location but are just chronological, so there's no way to group locations by IDs. I realized I needed to maintain coordinates for grouping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c018810c-aadd-44e1-8b82-86646503cf75",
   "metadata": {},
   "source": [
    "I added columns in Pandas for latitude and longitude by converting the coordinates column from an embedded list and then splitting the values in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87470059-9169-4a73-9378-a34cfdffb856",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_2_weeks['coordinates'] = last_2_weeks['coordinates'].apply(tuple)\n",
    "\n",
    "last_2_weeks['latitude'] = last_2_weeks['latitude'].astype(float)\n",
    "last_2_weeks['longitude'] = last_2_weeks['longitude'].astype(float)\n",
    "\n",
    "last_2_weeks['coordinates'] = last_2_weeks['coordinates'].astype(str)\n",
    "\n",
    "last_2_weeks[['longitude', 'latitude']] = last_2_weeks['coordinates'].str.strip('()').str.split(', ', expand=True)\n",
    "last_2_weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d044b1-1590-4857-82ea-8e38ccd45d95",
   "metadata": {},
   "source": [
    "I started looking into the number of locations per date, which I had earlier determined to be 50, and realized this differed based on time period. There were 45 locations up to June 29, 2024, which increased to 50 locations on June 30, 2024. This means some hotspot locations are occasionally added or changed. I saw the addition in the results from this code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20efca11-ed8e-4516-8c52-5fcf872f073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dates = df.groupby('date')['date'].value_counts().reset_index(name=\"locations_per_date\")\n",
    "\n",
    "df_dates[350:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcfbda8-5406-4a3f-a627-78673c5ca41f",
   "metadata": {},
   "source": [
    "I created a sub-dataset (dataframe) including the data only for the last two weeks. First, I filtered the dataframe by the last 700 rows, since there are 50 rows per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e3506-d7b5-40b4-8fb2-8a1235a691ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_2_weeks = df.sort_values(by='date', ascending=False).head(700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934842b6-1160-4539-9b1b-8c99b853ff7f",
   "metadata": {},
   "source": [
    "However, I realized that if locations are added, the number of rows per day would also change. I then tried to group by latitude and longitude and take the last 14 values for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090661b5-0adf-41eb-aa0f-5988b2d9d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_2_weeks = df.groupby(['longitude', 'latitude']).head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea78cf-7261-40c4-8cfc-77492e641195",
   "metadata": {},
   "source": [
    "But this also gave me incorrect data, because it appears some locations have also been removed from the list of hotspots, meaning their most recent values were months ago. I looked up another way to do it and came across using datetime and timedelta to specify the specific days I wanted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdce94e-b015-4351-a13f-0686a244a4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "today_date = df['date'].max()  \n",
    "two_weeks_ago = today_date - timedelta(days=14)\n",
    "\n",
    "last_2_weeks = df[(df['date'] > two_weeks_ago) & (df['date'] <= today_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed51b726-ccbb-4f2c-acf3-8fc20e090582",
   "metadata": {},
   "source": [
    "I calculated the **maximum flood risk on any day in the last two weeks**, converted this series to a dataframe, and merged this dataframe with my two-weeks dataframe to add maximum flood risk as a new column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb321740-0dce-42c5-b127-451ce2602279",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_risk_last_2_weeks = last_2_weeks.groupby('coordinates')['flood_risk'].max()\n",
    "max_risk_last_2_weeks_df = max_risk_last_2_weeks.reset_index(name=\"max_risk_last_2_weeks\")\n",
    "\n",
    "last_2_weeks = last_2_weeks.merge(max_risk_last_2_weeks_df, how='left', on='coordinates')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b5c2f-4428-4b35-b085-10ce421dcc82",
   "metadata": {},
   "source": [
    "I conducted a similar process to determine the **total of the flood risk values in the last two weeks** for each location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33d01c-7ebc-44ae-a4cc-f848b0208848",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_risk_last_2_weeks = last_2_weeks.groupby('coordinates')['flood_risk'].sum()\n",
    "sum_risk_last_2_weeks_df = sum_risk_last_2_weeks.reset_index(name=\"sum_risk_last_2_weeks\")\n",
    "\n",
    "last_2_weeks = last_2_weeks.merge(sum_risk_last_2_weeks_df, how='left', on='coordinates')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120852bd-a9fc-4a86-9170-a94078f3b180",
   "metadata": {},
   "source": [
    "Bringing the dataframe back to a Python list, I added in a column called **radius**, setting hotspots with a two weeks aggregate flood risk of 0 to 1 and hotspots with a two weeks aggregate flood risk of greater than 0 to (sum of risk + 1) * 3. I did this so that Datawrapper can recognize the size of all points (even those with a 0 flood risk value) and size by the total risk in the last two weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4934e799-cdfa-49e1-8d09-22a2411185da",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_2_weeks['radius'] = last_2_weeks['sum_risk_last_2_weeks'] + 1\n",
    "\n",
    "list_of_dicts = last_2_weeks.to_dict(orient='records')\n",
    "\n",
    "for hotspot in list_of_dicts:\n",
    "    if hotspot['radius'] == 1:\n",
    "        hotspot['radius'] = 2\n",
    "    elif hotspot['radius'] != 1:\n",
    "        hotspot['radius'] = hotspot['radius']*3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d616498a-69e0-429a-8087-0b75040ab133",
   "metadata": {},
   "source": [
    "Using the Google Maps API in Python, I **reverse Geocoded** the coordinates to find the names of each hotspot location. Then I add the locations to the dataframe. I change null values to 'Unknown.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741bf998-e0ad-482e-b32b-21030e2e83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_geocode(lat, lng, api_key):\n",
    "    url = f\"https://maps.googleapis.com/maps/api/geocode/json?latlng={lat},{lng}&key={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        if result['results']:\n",
    "            return result['results'][0]['formatted_address']\n",
    "        else:\n",
    "            return \"No address found\"\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b0ddbd-a188-4929-9ef6-62f9c71a37c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for location in list_of_dicts:\n",
    "    lat = location['latitude']\n",
    "    lng = location['longitude']\n",
    "    address = reverse_geocode(lat, lng, api_key)\n",
    "    location['address'] = address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce55431-7226-48df-b9b0-fec575e64254",
   "metadata": {},
   "source": [
    "(The below was done later once converting back to a dataframe and filtering for today's values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4d57c-2a8e-42ec-97ce-5a1425d488ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_df['hotspot_name'].fillna(\"Unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e204560-28a9-4437-900c-6b8328dcd143",
   "metadata": {},
   "source": [
    "Using datetime in Python, I change the date format in the date column to [Day of the Week], [Month] Day, Year. This was done during a revision of my Jupyter Notebook after I experienced challenges with automatic date formats in Datawrapper tooltips, which kept adding the time as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5692c7ca-bb44-41d1-8085-1f584527eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hotspot in list_of_dicts:\n",
    "\n",
    "    clean_date = hotspot['date'].strftime(\"%A, %B %d, %Y\")\n",
    "    hotspot['date'] = clean_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6289a330-49e0-4d63-bd14-84e9f28e0cdf",
   "metadata": {},
   "source": [
    "I cleaned the address column to get rid of extraneous text before the city and country name using regular expressions in Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f9ba3a-a9d8-44c0-ab42-dc01032489ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_2_weeks['address'] = last_2_weeks['address'].str.extract(r'.{7}\\s(.+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da0102e-123f-48a8-994f-a8e466eaf5e0",
   "metadata": {},
   "source": [
    "Finally, I renamed the address column to **hotspot_name** for easier understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92b4c6b-4857-4a73-904d-73463fb65075",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_2_weeks = last_2_weeks.rename(columns={'address': 'hotspot_name'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec449e-b329-455a-8a2a-89f7133ce802",
   "metadata": {},
   "source": [
    "I then created a dataframe only for today's date, first by extracting the first 50 rows from the revised dataframe, which represents the first instance of all 50 locations in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cf0d8f-9b1a-4ebc-919d-e2700b67bab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_df = last_2_weeks[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73ae3fd-9a51-4fbf-b0e9-1ac6ba605efe",
   "metadata": {},
   "source": [
    "But again, I realized this would differ if locations were added or subtracted. I changed the code to this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6ef952-2c77-4d6c-b389-87a463a59bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_df = last_2_weeks.groupby(['longitude','latitude'], as_index=False).last()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b2fd9a-b4f8-4c5a-820e-1eeae307e2f0",
   "metadata": {},
   "source": [
    "I did a bit of final cleaning of the today dataframe, removing the ID column and reordering the columns so the location name was first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab13368-35fd-4e8b-bf48-8b0afce0137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_df = today_df.drop(columns='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d265b86c-c823-489b-84d1-1bf35cf98a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = today_df.pop('hotspot_name')\n",
    "today_df.insert(0, 'hotspot_name', column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2321e716-0209-4968-a57c-03eb1fcd11df",
   "metadata": {},
   "source": [
    "I saved both dataframes to CSV (comma separated values) files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bde3ad-4f61-4e05-a5be-4d58ef1b5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_2_weeks.to_csv(\"flood_risk_last_2_weeks.csv\", index=False)\n",
    "\n",
    "today_df.to_csv(\"flood_risk_today.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dfcc80-bc8c-4404-86ef-4d6d561c742e",
   "metadata": {},
   "source": [
    "## Step 3: Auto-Updating the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c23cd9-f8cf-442a-bd58-9c217b8bfe86",
   "metadata": {},
   "source": [
    "I created a Github repo for this project: https://github.com/annikamcginnis/nile_basin_flood_hotspots_daily/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53ea775-a95b-4f3f-bbab-13b507664d82",
   "metadata": {},
   "source": [
    "In **Github Actions - Simple Workflow**, I created a .yml file (update.yml) that imports all the libraries I used in my data download and analysis, requesting the daily dataframe (\"flood_risk_today.csv\") to update three times every day through the Python Jupyter Notebook file \"hotspot_clusters_updater.ipynb.\" Although I only need updated data once per day, the three times request set at different times of the day allows for potential server-side issues with receiving a response at busy times of the day.\n",
    "\n",
    "For this, I modified the .yml file from jsoma's bad-air-cities tutorial: https://github.com/jsoma/bad-air-cities/blob/main/.github/workflows/update.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7046c638-5ea1-412a-b7c4-e4d811926a13",
   "metadata": {},
   "source": [
    "## Step 4: Creating Datawrapper Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37de346-37a8-48ae-9622-8a59cb1f45f9",
   "metadata": {},
   "source": [
    "I created a **Symbol map** in Datawrapper.de, linking an external dataset to the flood_risk_today.csv file on my repo: https://github.com/annikamcginnis/nile_basin_flood_hotspots_daily/flood_risk_today.csv. \n",
    "\n",
    "- I sized points by the radius values.\n",
    "- I colored points by the daily flash flood risk values, on a sliding scale.\n",
    "- I added a color and size legend.\n",
    "- I modified tooltips to include information upon hover for each hotspot's location, flood risk today, and maximum flood risk in the last two weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc70b5a2-bcc5-4f52-9823-763cd8d97094",
   "metadata": {},
   "source": [
    "## Step 5: Creating the Map Website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc1576c-6a73-4749-8ab6-539efdfc4b6b",
   "metadata": {},
   "source": [
    "I created an HTML website file (**index.html**) by modifying the index.html file from jsoma's bad-air-cities tutorial: https://github.com/jsoma/bad-air-cities/blob/main/index.html\n",
    "I inserted the responsive iFrame embed code from my Datawrapper map.\n",
    "I modified the CSS to extend the map across the screen and tailor the page design (words, font, background color, etc).\n",
    "I published the index.html file using Github Pages.\n",
    "\n",
    "**Final map URL: https://annikamcginnis.github.io/nile_basin_flood_hotspots_daily/**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819eb3a1-1488-496a-9882-c47291c5f4fe",
   "metadata": {},
   "source": [
    "_I wanted to create a search bar where a user could search for a location and be told how close they are to a flash flood hotspot or a place that has experienced flood risk in the last few weeks, but this was outside the capabilities of Datawrapper. I can possibly explore doing this using other mapping approaches in the future._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
